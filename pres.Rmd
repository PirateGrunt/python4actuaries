---
title: "Introduction to Python for Actuaries"
author: [John Bogaardt, Brian A. Fannin ACAS CSPA]
date: March 20, 2018
output: 
  revealjs::revealjs_presentation:
    css: ./css/revealOpts.css
    center: no
    self_contained: no
    transition: slide
---

# What is Python? 

## What is Python?

Python is a programming language invented by ...

## ... this guy

![guido von rossum image](images/guido_von_rossum.jpg)

##

... and he named it Python because ...

## ... he was into Monty Python when he wrote it

![monty python image](images/monty_python.jpg)

## Zen of Python

* Beautiful is better than ugly.
* Explicit is better than implicit.
* Simple is better than complex.
* Complex is better than complicated.
* Flat is better than nested.
* Sparse is better than dense.
* Readability counts.
* Special cases aren't special enough to break the rules.
* Although practicality beats purity.
* Errors should never pass silently.
* Unless explicitly silenced.
* In the face of ambiguity, refuse the temptation to guess.
* There should be one-- and preferably only one --obvious way to do it.

## The ecosystem

TODO: 

* stack-overflow questions
* Events

## Similarities between R and Python

* FLOSS - Free, Libre, Open Source Software
* Wide support (take that, Julia!)
* Interpreted, REPL
* Rich package ecosystem
* Not OS dependent
* Fast to program
* Execution not always as fast as C, but possible to use C routines when needed
* great database support - from RDBMS to Spark, Hadoop, etc.

## How does it compare with R?

<div class='left'>
R ...

* loves statistics
* hasn't really settled on OOp
* vector support out of the box
* does anyone really use `try()`?
* has some actuarial packages
* RStudio!!!
* lacks consensus around machine learning - H2O, caret, ROCR?
</div>

<div class='right'>
Python ...

* is general purpose
* has easy and consistent support for OOP
* vectors are available in `numpy`
* great exception handling
* not much actuarial focus
* no consensus on a FLOSS IDE
* scikit-learn!
</div>

## Compare with Excel?

<div class='left'>
Excel ...

* Closed source
* Data is visible, but logic is hidden
* 
</div>

<div class='right'>
Python ...

* FLOSS
* Logic is visible, data is abstract
</div>

# How can I incorporate Python into my workflow?

## Practicalities

* Editor?
* Version 2 or 3?

## Some of the popular packages?

* numpy
* pandas
* 

## Python database packages ?

## Jupyter

TODO: A word about Jupyter

<!-- ## Use Excel only to communicate with a database -->

# Intro scikit learn

## scikit-learn

* Built on NumPy, SciPy, and matplotlib
* Open source, commercially usable - BSD license
* Common functional interface for:
    * Data splits, cross validations
    * Data transforms
    * Training, test, scoring
    * Pipelines to manage workflow

## But what algorithms does it support?

![algorithms cheat sheet](images/algorithm_cheat_sheet.png)

## She fits, she scores!

```
from sklearn.linear_model as LinearRegression()

linear_regression = LinearRegression()

linear_regression.fit(X_train, y_train)

r_square = linear_regression.score(X_test,y_test)

print('Model R_Square on holdout: ' + str(round(r_square,2)))
coef = pd.DataFrame(np.array([boston['feature_names'], 
                              linear_regression.coef_.round(4)])).T
coef.columns=(['Feature','Coefficient'])
coef
```

## Cross validation? Don't mind if I do.

```
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

boston = load_boston()

X_train, X_test, y_train, y_test = \
    train_test_split(boston['data'], boston['target'], 
                     test_size=0.33, random_state=42)
```

## Standardized across algorithms

```
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor

techniques = [LinearRegression(), Ridge(max_iter=100000),  
                 Lasso(max_iter=100000), ElasticNet(max_iter=100000), 
                 RandomForestRegressor(random_state=42), 
                 GradientBoostingRegressor(random_state=42),
                 DecisionTreeRegressor(random_state=42)]
model_names = [type(model).__name__ for model in linear_models]

fitted_models = [model.fit(X_train, y_train) for model in techniques]

model_r2 = [model.score(X_test,y_test) for model in fitted_models]

results = pd.DataFrame([model_names, model_r2], index=['Model','R2 Initial']).T
results
```

## Estimators

## Transformers

## Grid Search

```
from sklearn.model_selection import GridSearchCV
dtree = DecisionTreeRegressor(random_state=42)
pipe = Pipeline(steps=[('poly', poly), ('dtree', dtree)])

param_grid = dict(dtree__criterion =['mse','friedman_mse'],
                  dtree__max_depth = [25, 50, 75, 100],
             dtree__min_samples_leaf =[2, 3, 5, 10],
             dtree__min_samples_split = [5,10,20],
             poly__degree=[1,2])

optimized_pipeline = GridSearchCV(pipe, param_grid, cv=5, refit=True)
optimized_pipeline.fit(X_train, y_train)
holdout_score = round(optimized_pipeline.score(X_test,y_test),2)

print('Optimal parameter set produces a holdout score of ' + str(holdout_score) + '.')
pd.DataFrame(optimized_pipeline.best_params_,index=['Value']).T
```

## Pipelines

```
from sklearn.pipeline import Pipeline

poly = PolynomialFeatures(2)
lasso = lm.Lasso(max_iter=100000)

pipe = Pipeline(steps=[('poly', poly), ('lasso', lasso)])
pipe.fit(X_train, y_train)

print('A pipeline for our Lasso model produces a holdout score of ' \
      + str(round(pipe.score(X_test, y_test),2)) + '.')
```

# How about an example?

## Intro data

TODO: Talk about the data here

## sci-kit learn

# Wrapping up

## Python and you

* If you already know R, check out Python
* If you're ready for something beyond spreadsheets, Python is a great place to start
* scikit-learn makes machine learning easy

## Thank you for your time!

## Questions?

## References

* http://scikit-learn.org/stable/
* https://www.python.org/
* https://www.python.org/dev/peps/pep-0020/

<!-- 
Section 1 - 20 minutes
Section 2 - 20 minutes
Section 3 - 30 minutes
Conclusion - 5 minutes
Q & A - 15 minutes
-->

<!-- 20 minutes -->

