{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "\n",
    "Note the difference between `import` and `from` x `import` y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'feature_names', 'target', 'DESCR']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(boston.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], \n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=506, minmax=(5.0, 50.0), mean=22.532806324110677, variance=84.586723594098558, skewness=1.104810822864635, kurtosis=1.4686287722747462)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "y = boston['target']\n",
    "\n",
    "stats.describe(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL6/nW5OO76OKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb384e82ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "          boston['data']\n",
    "        , boston['target']\n",
    "        , test_size=0.33\n",
    "        , random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression example\n",
    "\n",
    "ESTIMATOR.fit(predictors, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "linear_regression = lm.LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model R_Square on the holdout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72585158182300469"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAF note: is there a summary function in scikit-learn that would work better here? I'm thinking of something analogous to R's `summary()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIM</td>\n",
       "      <td>-0.12806039830227736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZN</td>\n",
       "      <td>0.037795569275776555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INDUS</td>\n",
       "      <td>0.05861077967791607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHAS</td>\n",
       "      <td>3.240070073436767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOX</td>\n",
       "      <td>-16.222267596612692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RM</td>\n",
       "      <td>3.8935224412487948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AGE</td>\n",
       "      <td>-0.012787994352285318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DIS</td>\n",
       "      <td>-1.4232686401061765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RAD</td>\n",
       "      <td>0.23451308179194089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TAX</td>\n",
       "      <td>-0.008202611267236669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PTRATIO</td>\n",
       "      <td>-0.9299505351901204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B</td>\n",
       "      <td>0.011915140990558872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTAT</td>\n",
       "      <td>-0.5484899972520091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature            Coefficient\n",
       "0      CRIM   -0.12806039830227736\n",
       "1        ZN   0.037795569275776555\n",
       "2     INDUS    0.05861077967791607\n",
       "3      CHAS      3.240070073436767\n",
       "4       NOX    -16.222267596612692\n",
       "5        RM     3.8935224412487948\n",
       "6       AGE  -0.012787994352285318\n",
       "7       DIS    -1.4232686401061765\n",
       "8       RAD    0.23451308179194089\n",
       "9       TAX  -0.008202611267236669\n",
       "10  PTRATIO    -0.9299505351901204\n",
       "11        B   0.011915140990558872\n",
       "12    LSTAT    -0.5484899972520091"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "      np.array([boston['feature_names'], linear_regression.coef_]).T\n",
    "    , columns = ['Feature','Coefficient']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each estimator in `sklearn` has  `fit()`, `predict()`, and `score()` methods. This makes swapping techniques easy.\n",
    "\n",
    "ESTIMATOR.fit(predictors, response)\n",
    "ESTIMATOR.predict(predictors, response)\n",
    "ESTIMATOR.score(predictors, response)\n",
    "\n",
    "Let's fit an OLS, ridge, lasso, elasticnet, random forest, gradient boost and decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAF: In the data frame that we're printing, can we sort the models in descending order by R^2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2 Initial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.725852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.720131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.664381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.668769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.812672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>0.893198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>0.740223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model R2 Initial\n",
       "0           LinearRegression   0.725852\n",
       "1                      Ridge   0.720131\n",
       "2                      Lasso   0.664381\n",
       "3                 ElasticNet   0.668769\n",
       "4      RandomForestRegressor   0.812672\n",
       "5  GradientBoostingRegressor   0.893198\n",
       "6      DecisionTreeRegressor   0.740223"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.ensemble as ens\n",
    "import sklearn.tree as tree\n",
    "\n",
    "max_iter = 100000\n",
    "random_state = 42\n",
    "\n",
    "linear_models = [\n",
    "      lm.LinearRegression() \n",
    "    , lm.Ridge(max_iter = max_iter)\n",
    "    , lm.Lasso(max_iter = max_iter)\n",
    "    , lm.ElasticNet(max_iter = max_iter)\n",
    "    , ens.RandomForestRegressor(random_state=random_state)\n",
    "    , ens.GradientBoostingRegressor(random_state=random_state)\n",
    "    , tree.DecisionTreeRegressor(random_state=random_state)\n",
    "]\n",
    "\n",
    "model_names = [type(model).__name__ for model in linear_models]\n",
    "\n",
    "fitted_models = [model.fit(X_train, y_train) for model in linear_models]\n",
    "\n",
    "model_r2 = [model.score(X_test,y_test) for model in fitted_models]\n",
    "\n",
    "results = pd.DataFrame([model_names, model_r2], index=['Model','R2 Initial']).T\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The linear models do not fare as well as the ensemble techniques.  This is likely due to non-linear relationships in the underlying featureset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocess data\n",
    "\n",
    "Scikit-learn has a variety of transformers that take raw data and generate transformations that are better suited for certain algorithms.\n",
    "\n",
    "These transformers include StandardScaler(), PCA(), Imputer(), LabelBinarizer(), PolynomialFeature(), and so much more...\n",
    "\n",
    "Transformers are estimators too and have fit() methods to set up the transformation as well as transform() method to apply it to new data.\n",
    "\n",
    "*** figure out a sexy way to show how comprehensive the toolkit is in feature engineering/data preprocessing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly.fit(X_train)\n",
    "\n",
    "X_train_transform = poly.transform(X_train)\n",
    "X_test_transform = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By adding 2nd degree terms to our feature set we expand our featureset from 13 features to 105.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2 Initial</th>\n",
       "      <th>R2 Poly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.725852</td>\n",
       "      <td>0.486611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.720131</td>\n",
       "      <td>0.662576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.664381</td>\n",
       "      <td>0.838244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.668769</td>\n",
       "      <td>0.842125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.812672</td>\n",
       "      <td>0.825092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>0.893198</td>\n",
       "      <td>0.887188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>0.740223</td>\n",
       "      <td>0.74719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model R2 Initial   R2 Poly\n",
       "0           LinearRegression   0.725852  0.486611\n",
       "1                      Ridge   0.720131  0.662576\n",
       "2                      Lasso   0.664381  0.838244\n",
       "3                 ElasticNet   0.668769  0.842125\n",
       "4      RandomForestRegressor   0.812672  0.825092\n",
       "5  GradientBoostingRegressor   0.893198  0.887188\n",
       "6      DecisionTreeRegressor   0.740223   0.74719"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('By adding 2nd degree terms to our feature set we expand our featureset from ' + str(len(boston['feature_names'])) + \\\n",
    "      ' features to ' + str(len(poly.get_feature_names())) + '.')\n",
    "\n",
    "fitted_models = [model.fit(X_train_transform,y_train) for model in linear_models]\n",
    "\n",
    "model_r2 = [model.score(X_test_transform,y_test) for model in fitted_models]\n",
    "\n",
    "results = results.T.append(pd.Series(model_r2, name='R2 Poly')).T\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn is customizable\n",
    "\n",
    "You can even work your own transformers into the workflow.  Let's explore taking the natural log of our featureset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2 Initial</th>\n",
       "      <th>R2 Poly</th>\n",
       "      <th>R2 Log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.725852</td>\n",
       "      <td>0.486611</td>\n",
       "      <td>0.822088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.720131</td>\n",
       "      <td>0.662576</td>\n",
       "      <td>0.786281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.664381</td>\n",
       "      <td>0.838244</td>\n",
       "      <td>0.664378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.668769</td>\n",
       "      <td>0.842125</td>\n",
       "      <td>0.669214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.812672</td>\n",
       "      <td>0.825092</td>\n",
       "      <td>0.824317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>0.893198</td>\n",
       "      <td>0.887188</td>\n",
       "      <td>0.895237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>0.740223</td>\n",
       "      <td>0.74719</td>\n",
       "      <td>0.727383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model R2 Initial   R2 Poly    R2 Log\n",
       "0           LinearRegression   0.725852  0.486611  0.822088\n",
       "1                      Ridge   0.720131  0.662576  0.786281\n",
       "2                      Lasso   0.664381  0.838244  0.664378\n",
       "3                 ElasticNet   0.668769  0.842125  0.669214\n",
       "4      RandomForestRegressor   0.812672  0.825092  0.824317\n",
       "5  GradientBoostingRegressor   0.893198  0.887188  0.895237\n",
       "6      DecisionTreeRegressor   0.740223   0.74719  0.727383"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "\n",
    "X_train_transform = np.append(X_train,transformer.transform(X_train), axis=1)\n",
    "X_test_transform = np.append(X_test,transformer.transform(X_test), axis=1)\n",
    "\n",
    "fitted_models = [model.fit(X_train_transform,y_train) for model in linear_models]\n",
    "\n",
    "model_r2 = [model.score(X_test_transform,y_test) for model in fitted_models]\n",
    "\n",
    "results = results.T.append(pd.Series(model_r2, name='R2 Log')).T\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-learn is a scalable workflow\n",
    "\n",
    "### Pipelining\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.  Pipelines are estimators too with fit(), predict() and score() methods. \n",
    "\n",
    "\n",
    "DATA --> TRANSFORM --> FIT\n",
    "\n",
    "An example of a simple pipeline chaining polynomial transform and lasso regression together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout score:0.838244117471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "lasso = lm.Lasso(max_iter=100000)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps = [('poly', poly), ('lasso', lasso)]\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print('Holdout score:' + str(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pipelining with GridSearchCV\n",
    "\n",
    "The real power of pipelines is realized when using them with GridSearchCV - The beauty here is that both transformers and the model maintain data separability between the train and test of each fold in the cross-validation routine.  This ensures no data leakage in evaluating estimator performance, but allows us to refine the hyperparameters of all estimators in the pipeline.\n",
    "\n",
    "### Optimize our DecisionTreeRegressor\n",
    "\n",
    "DATA --> TRANSFORM --> SPLIT --> GRID_SEARCH.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "PandasError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPandasError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fca89bb05f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mcv_test_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    301\u001b[0m                                          copy=False)\n\u001b[1;32m    302\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPandasError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPandasError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dtree = tree.DecisionTreeRegressor(random_state = random_state)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "          ('poly', poly)\n",
    "        , ('dtree', dtree)\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = dict(\n",
    "      dtree__criterion = ['mse','friedman_mse']\n",
    "    , dtree__max_depth = [25, 50, 75, 100]\n",
    "    , dtree__min_samples_leaf = [2, 3, 5, 10]\n",
    "    , dtree__min_samples_split = [5,10,20]\n",
    "    , poly__degree = [1,2])\n",
    "\n",
    "estimator = GridSearchCV(pipe, param_grid, return_train_score = True, cv = 5)\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "parameters = pd.DataFrame(estimator.cv_results_['params'])\n",
    "\n",
    "cv_test_scores = pd.Series(estimator.cv_results_['mean_test_score'],name='mean_test_score')\n",
    "\n",
    "parameters.T.append(cv_test_scores).T.sort_values('mean_test_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrain the decision tree on our training set with the optimal hyper-parameters from the GridSearch space and verify holdout score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An optimized pipeline for our Decision Tree Regressor produces a holdout score of 0.819812808935\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "dtree = tree.DecisionTreeRegressor(\n",
    "      criterion = 'friedman_mse'\n",
    "    , max_depth = 50\n",
    "    , min_samples_leaf = 3\n",
    "    , min_samples_split = 20\n",
    "    , random_state = random_state\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "          ('poly', poly)\n",
    "        , ('dtree', dtree)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print('An optimized pipeline for our Decision Tree Regressor produces a holdout score of ' + str(pipe.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
